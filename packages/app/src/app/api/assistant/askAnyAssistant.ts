'use server'

import { ClapWorkflowProvider } from '@aitube/clap'
import { RunnableLike } from '@langchain/core/runnables'
import { ChatPromptValueInterface } from '@langchain/core/prompt_values'
import {
  AIMessage,
  AIMessageChunk,
  HumanMessage,
} from '@langchain/core/messages'
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from '@langchain/core/prompts'
import { ChatOpenAI } from '@langchain/openai'
import { ChatGroq } from '@langchain/groq'
import { ChatAnthropic } from '@langchain/anthropic'
import { ChatCohere } from '@langchain/cohere'
import { ChatMistralAI } from '@langchain/mistralai'
import { ChatVertexAI } from '@langchain/google-vertexai'
// to properly support Replicate and Hugging Face we need the following packages:
// import { ChatReplicate } from "@langchain/replicate"
// import { ChatHuggingFace } from "@langchain/huggingface"

import {
  AssistantInput,
  AssistantAction,
  AssistantMessage,
  AssistantRequest,
  AssistantSceneSegment,
  AssistantStoryBlock,
  ChatEventVisibility,
} from '@aitube/clapper-services'

import { examples, humanTemplate, systemTemplate } from './templates'
import { isValidNumber } from '@/lib/utils'
import { assistantMessageParser, formatInstructions } from './parser'
import { parseRawInputToAction } from '@/services/assistant/parseRawInputToAction'
import { parseLangChainResponse } from './parseLangChainResponse'
import { getApiKey } from '../getApiKey'
import {
  builtinProviderCredentialsAnthropic,
  builtinProviderCredentialsCohere,
  builtinProviderCredentialsGoogle,
  builtinProviderCredentialsGroq,
  builtinProviderCredentialsMistralai,
  builtinProviderCredentialsOpenai,
} from '../globalSettings'

/**
 * Query the preferred language model on the user prompt + the segments of the current scene
 *
 * @param userPrompt
 * @param segments
 * @returns
 */
export async function askAnyAssistant({
  settings,

  prompt,

  // the slice to edit
  segments = [],

  fullScene = '',

  actionLine = '',

  // used to provide more context
  entities = {},

  // used to provide more context
  projectInfo = '',

  history = [],
}: AssistantRequest): Promise<AssistantMessage> {
  const workflow = settings.assistantWorkflow
  const provider = workflow.provider
  const modelName = workflow.data

  console.log('askAnyAssistant: provider =', provider, 'model =', modelName)
  console.log(
    'askAnyAssistant: groqApiKey length =',
    settings.groqApiKey?.length || 0
  )

  if (!provider) {
    throw new Error(`Missing assistant provider`)
  }

  // Enhanced model selection with fallback options
  const coerceable: RunnableLike<ChatPromptValueInterface, AssistantMessage> =
    provider === ClapWorkflowProvider.GROQ
      ? (() => {
          const apiKey = getApiKey(
            settings.groqApiKey,
            builtinProviderCredentialsGroq,
            settings.clapperApiKey
          )
          console.log(
            'askAnyAssistant: final apiKey length =',
            apiKey?.length || 0
          )

          // Try alternative models if primary has issues
          const models = [
            'llama3-70b-8192',
            'llama-3.1-70b-versatile',
          ]
          const currentModel = modelName || models[0]

          console.log(
            'askAnyAssistant: creating ChatGroq with model =',
            currentModel
          )
          return new ChatGroq({
            apiKey,
            modelName: currentModel,
            temperature: 0.1, // Lower temperature for more consistent formatting
            maxTokens: 2048,
            // Add retry configuration
            maxRetries: 2,
          })
        })()
      : provider === ClapWorkflowProvider.OPENAI
        ? new ChatOpenAI({
            openAIApiKey: getApiKey(
              settings.openaiApiKey,
              builtinProviderCredentialsOpenai,
              settings.clapperApiKey
            ),
            modelName,
            // temperature: 0.7,
          })
        : provider === ClapWorkflowProvider.ANTHROPIC
          ? new ChatAnthropic({
              anthropicApiKey: getApiKey(
                settings.anthropicApiKey,
                builtinProviderCredentialsAnthropic,
                settings.clapperApiKey
              ),
              modelName,
              // temperature: 0.7,
            })
          : provider === ClapWorkflowProvider.COHERE
            ? new ChatCohere({
                apiKey: getApiKey(
                  settings.cohereApiKey,
                  builtinProviderCredentialsCohere,
                  settings.clapperApiKey
                ),
                model: modelName,
                // temperature: 0.7,
              })
            : provider === ClapWorkflowProvider.MISTRALAI
              ? new ChatMistralAI({
                  apiKey: getApiKey(
                    settings.mistralAiApiKey,
                    builtinProviderCredentialsMistralai,
                    settings.clapperApiKey
                  ),
                  modelName,
                  // temperature: 0.7,
                })
              : provider === ClapWorkflowProvider.GOOGLE
                ? new ChatVertexAI({
                    apiKey: getApiKey(
                      settings.googleApiKey,
                      builtinProviderCredentialsGoogle,
                      settings.clapperApiKey
                    ),
                    modelName,
                    // temperature: 0.7,
                  })
                : undefined

  if (!coerceable) {
    throw new Error(
      `Provider ${provider} is not supported yet. If a LangChain bridge exists for this provider, then you can add it to Clapper.`
    )
  }

  const chatPrompt = ChatPromptTemplate.fromMessages([
    ['system', systemTemplate],
    new MessagesPlaceholder('chatHistory'),
    ['human', humanTemplate],
  ])

  //const storyBlocks: AssistantStorySentence[] = fullScene.split(/(?:. |\n)/).map(storySentence => {
  //})

  const storyBlocks: AssistantStoryBlock[] = [
    {
      blockId: 0,
      block: fullScene,
    },
    {
      blockId: 1,
      block: actionLine,
    },
  ]

  // we don't give the whole thing to the LLM as to not confuse it,
  // and also to keep things tight and performant
  const sceneSegments: AssistantSceneSegment[] = segments.map((segment, i) => ({
    segmentId: i,
    prompt: segment.prompt,
    startTimeInMs: segment.startTimeInMs,
    endTimeInMs: segment.endTimeInMs,
    category: segment.category,
  }))

  // TODO put this into a type
  const inputData: AssistantInput = {
    directorRequest: prompt,
    storyBlocks,
    sceneSegments,
  }

  // console.log("INPUT:", JSON.stringify(inputData, null, 2))

  const chain = chatPrompt.pipe(coerceable)
  // .pipe(assistantMessageParser)  // temporarily disable structured parsing


  let assistantMessage: AssistantMessage = {
    comment: '',
    action: AssistantAction.NONE,
    updatedStoryBlocks: [],
    updatedSceneSegments: [],
  }

  // For simple greetings, return a friendly response without calling the LLM
  const simpleGreetings = ['hello', 'hi', 'hey', 'test']
  if (
    simpleGreetings.some((greeting) => prompt.toLowerCase().includes(greeting))
  ) {
    assistantMessage.comment = `Hello! I'm your AI assistant, ready to help with your video production! üé¨

I can assist you with:
‚Ä¢ **Script Analysis**: Break down your script into scenes
‚Ä¢ **Scene Creation**: Turn text into visual descriptions  
‚Ä¢ **Video Planning**: Structure scenes for production
‚Ä¢ **Creative Guidance**: Add cinematography and direction notes

What would you like to work on? Try asking:
- "Help me create a scene"  
- "Break down this script section"
- "Add visual details to this dialogue"`
    return assistantMessage
  }

  // --- LLM call and logging ---
  let rawLlmResponse: any = null
  let llmError: any = null
  try {
    // Convert chat history to LangChain message objects
    const lcChatHistory = Array.isArray(history)
      ? history.map((msg) => {
          if (!msg || typeof msg !== 'object' || !msg.senderId || !msg.message) return null
          if (msg.senderId === 'director' || msg.isCurrentUser) {
            return new HumanMessage(msg.message)
          } else if (msg.senderId === 'assistant') {
            return new AIMessage(msg.message)
          }
          return null
        }).filter(Boolean)
      : []
    // Log input data for debugging
    console.log('[askAnyAssistant] LLM inputData:', JSON.stringify(inputData, null, 2))
    // Run the LLM chain
    rawLlmResponse = await chain.invoke({
      chatHistory: lcChatHistory,
      inputData: JSON.stringify(inputData),
    })
    console.log('[askAnyAssistant] LLM raw response:', JSON.stringify(rawLlmResponse, null, 2))
    // Parse the LLM response
    assistantMessage = parseLangChainResponse(rawLlmResponse)
    console.log('[askAnyAssistant] Parsed assistantMessage:', JSON.stringify(assistantMessage, null, 2))
    // If the response is valid, return it
    if (assistantMessage && (assistantMessage.comment || assistantMessage.updatedSceneSegments?.length || assistantMessage.updatedStoryBlocks?.length)) {
      return assistantMessage
    }
  } catch (err) {
    llmError = err
    console.error('[askAnyAssistant] LLM error:', err)
  }
  // --- End LLM call and logging ---

  // Patch: If fallback is triggered, log and return the raw LLM output for debugging
  console.warn('[askAnyAssistant] Fallback triggered. Returning raw LLM output for debugging:')
  console.warn('[askAnyAssistant] rawLlmResponse:', JSON.stringify(rawLlmResponse, null, 2))
  assistantMessage.comment = '[DEBUG] Fallback triggered. Raw LLM output below.\n' + JSON.stringify(rawLlmResponse, null, 2)
  return assistantMessage

  // The following LLM processing code will activate automatically once formatting issues resolve

  try {
    const rawResponse = await chain.invoke({
      formatInstructions,
      examples,
      projectInfo,
      inputData: JSON.stringify(inputData),

      // we don't use this capability yet, but we can create a "fake"
      // chat history that will contain JSON and will only be shown to the AI
      // by using the `visibility` setting
      chatHistory: history
        .filter(
          (event) => event.visibility !== ChatEventVisibility.TO_USER_ONLY
        )
        .map(
          ({
            eventId,
            senderId,
            senderName,
            roomId,
            roomName,
            sentAt,
            message,
            isCurrentUser,
            visibility,
          }) => {
            if (isCurrentUser) {
              return new HumanMessage(message)
            } else {
              return new AIMessage(message)
            }
          }
        ),
    })
    console.log('askAnyAssistant: raw response from LLM:')
    console.log(JSON.stringify(rawResponse, null, 2))
    console.log('askAnyAssistant: attempting to parse response...')

    assistantMessage = parseLangChainResponse(rawResponse as any)
    console.log('askAnyAssistant: parsing successful!')
    // console.log('assistantMessage:', assistantMessage)
  } catch (err) {
    // LangChain failure (this happens quite often, actually)

    // console.log(`Langchain error:\n${err}`)
    let errorPlainText = `${err}`

    // Markdown formatting failure
    errorPlainText = `${errorPlainText.split('```').unshift() || errorPlainText}`

    // JSON parsing exception failure
    errorPlainText =
      errorPlainText.split(`Error: Failed to parse. Text: "`).pop() ||
      errorPlainText

    errorPlainText =
      errorPlainText.split(`". Error: SyntaxError`).shift() || errorPlainText

    if (errorPlainText) {
      console.log(
        `failed to parse the response from the LLM, trying to repair the output from LangChain..`
      )
      console.log('Raw error response that failed to parse:')
      console.log(errorPlainText)

      try {
        assistantMessage = parseLangChainResponse(JSON.parse(errorPlainText))
      } catch (err) {
        console.log(`repairing the output failed!`, err)
        console.log('Final parsed errorPlainText:', errorPlainText)

        // Enhanced fallback responses based on request type
        if (
          prompt.toLowerCase().includes('script') ||
          prompt.toLowerCase().includes('scene') ||
          prompt.toLowerCase().includes('create')
        ) {
          assistantMessage.comment = `üé¨ **Script & Scene Assistant Ready!**

I received your request about scripts/scenes! While working around current formatting issues, I can guide you through:

**üìù Script Breakdown Process:**
‚Ä¢ Share your script text and I'll identify key scenes
‚Ä¢ Describe a moment and I'll add visual details
‚Ä¢ Ask for specific help: "How should I film [situation]?"

**üé• Scene Creation Steps:**
1. **Story Moment**: Describe what happens
2. **Visual Style**: Camera angles, lighting mood  
3. **Production Notes**: Practical filming guidance

**Example Requests:**
‚Ä¢ "Break down this dialogue into a visual scene"
‚Ä¢ "Add camera directions to this action sequence"  
‚Ä¢ "Help me plan the opening scene"

**Status**: Smart fallbacks active while LLM formatting stabilizes. Your creative work continues uninterrupted!

Ready to collaborate on your vision! üéØ`
        } else {
          assistantMessage.comment = `ü§ñ **AI Assistant Status Update**

I'm actively processing your request! Currently using enhanced fallback responses while resolving LLM formatting issues.

**üìä Technical Details:**
‚Ä¢ **Connection**: ‚úÖ API connected successfully  
‚Ä¢ **Processing**: ‚úÖ Request received and parsed
‚Ä¢ **Response Format**: ‚ö†Ô∏è Temporary parsing issues
‚Ä¢ **Fallback System**: ‚úÖ Smart responses active

**üé¨ Available Support:**
‚Ä¢ Video production guidance
‚Ä¢ Script and scene development  
‚Ä¢ Creative direction assistance
‚Ä¢ Timeline and project planning

**üí° Try These:**
‚Ä¢ Simple requests: "Hello", "Help me start"
‚Ä¢ Specific questions: "How do I film [scenario]?"
‚Ä¢ Creative input: "Create a scene where..."

Your assistant is ready - let's create something amazing! üöÄ`
        }
        assistantMessage.action = AssistantAction.NONE
        assistantMessage.updatedSceneSegments = []
        assistantMessage.updatedStoryBlocks = []
        if (!errorPlainText) {
          throw new Error(
            `failed to repair the output from LangChain (empty string)`
          )
        }
      }
    } else {
      throw new Error(
        `couldn't process the request or parse the response (${err})`
      )
    }
  }

  return assistantMessage
}
